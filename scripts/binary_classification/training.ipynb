{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fonts and fontsize for plotting\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['mathtext.fontset'] = 'dejavuserif'\n",
    "fontsize = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to randomly shuffle the data\n",
    "def shuffle_data(data, labels, SEED):\n",
    "    \n",
    "    np.random.seed(SEED)\n",
    "    \n",
    "    np.random.shuffle(data)\n",
    "    np.random.shuffle(labels)\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the data into training, validation, and testing set\n",
    "def split_data(data, labels):\n",
    "    \n",
    "    # Split the data into training, validation, and testing set in ratio 80:10:10\n",
    "    # Training set\n",
    "    data_train = data[:int(0.8*len(data)), :, :]\n",
    "    labels_train = labels[:int(0.8*len(labels))]\n",
    "    \n",
    "    # Validation set\n",
    "    data_val = data[int(0.8*len(data)):int(0.9*len(data)), :, :]\n",
    "    labels_val = labels[int(0.8*len(labels)):int(0.9*len(labels))]\n",
    "    \n",
    "    # Testing set\n",
    "    data_test = data[int(0.9*len(data)):, :, :]\n",
    "    labels_test = labels[int(0.9*len(labels)):]\n",
    "        \n",
    "    return data_train, labels_train, data_val, labels_val, data_test, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN class\n",
    "class CNN():\n",
    "\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        \n",
    "        # Initialize input_layer here\n",
    "        self.input_layer = None  \n",
    "\n",
    "    # Method to build the hidden layers\n",
    "    def build_hidden_layers(self):\n",
    "        \n",
    "        # Convolutional Layers\n",
    "        # First Convolutional Layer\n",
    "        x1 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding='same', kernel_initializer = 'glorot_normal')(self.input_layer)\n",
    "        x1 = tf.keras.layers.Activation('relu')(x1)\n",
    "        x1 = tf.keras.layers.BatchNormalization()(x1)\n",
    "\n",
    "        # Second Convolutional Layer\n",
    "        x2 = tf.keras.layers.Conv2D(filters=90, kernel_size=(3,3), padding='same', kernel_initializer = 'glorot_normal')(x1)\n",
    "        x2 = tf.keras.layers.Activation('relu')(x2)\n",
    "        x2 = tf.keras.layers.BatchNormalization()(x2)\n",
    "\n",
    "        return x2\n",
    "\n",
    "    # Method to build the overall model\n",
    "    def build_model(self):\n",
    "        \n",
    "        # Input layer\n",
    "        self.input_layer = tf.keras.layers.Input(shape=self.input_shape)\n",
    "\n",
    "        # Hidden layer\n",
    "        hidden_layer = self.build_hidden_layers()\n",
    "\n",
    "        # Add a flatten layer\n",
    "        flatten_layer = tf.keras.layers.Flatten()(hidden_layer)\n",
    "\n",
    "        # Output Layer\n",
    "        output_layer = tf.keras.layers.Dense(units=self.output_shape, activation='sigmoid')(flatten_layer)\n",
    "\n",
    "        # Build model\n",
    "        self.model = tf.keras.models.Model(inputs=[self.input_layer], outputs=[output_layer])\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    # Method to compile the model\n",
    "    def compile(self, optimizer, loss):\n",
    "        \n",
    "        # Compile model\n",
    "        self.model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "        return self.model\n",
    "    \n",
    "    # Define method to train the model\n",
    "    def train(self, x_train, y_train, x_val, y_val, epochs, batch_size, callbacks):\n",
    "        \n",
    "        # Train model\n",
    "        self.history = self.model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=callbacks,\n",
    "                                      validation_data=(x_val, y_val))\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    # Method to print summary of model\n",
    "    def summary(self):\n",
    "        \n",
    "        self.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class plots:\n",
    "    \n",
    "    def __init__(self, history, file_directory):\n",
    "\n",
    "        self.history = history\n",
    "        self.file_directory = file_directory\n",
    "\n",
    "    def loss(self):\n",
    "\n",
    "        loss_name = list(self.history.history.keys())[0]\n",
    "\n",
    "        # Training\n",
    "        loss = self.history.history[loss_name]\n",
    "        val_loss = self.history.history['val_' + loss_name]\n",
    "\n",
    "        loss_plot = plt.figure()\n",
    "        epochs = range(1, len(loss)+1)\n",
    "        plt.plot(epochs, loss, 'bo--', label = 'Training Loss', markersize = 2)\n",
    "        plt.plot(epochs, val_loss, 'go--', label = 'Validation Loss', markersize = 2)\n",
    "        plt.title('Training and Validation Loss', fontsize=fontsize)\n",
    "        plt.xlabel('Epochs', fontsize=fontsize)\n",
    "        plt.ylabel('Loss', fontsize=fontsize)\n",
    "        plt.legend(['Training Loss', 'Validation Loss'], fontsize=fontsize)\n",
    "        ax = loss_plot.gca()\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        plt.savefig(self.file_directory + '/loss.pdf', bbox_inches='tight')\n",
    "        \n",
    "        return loss_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "grandparent_directory = os.path.dirname(parent_directory)\n",
    "data_directory = os.path.join(grandparent_directory, 'data')\n",
    "npy_files_directory = os.path.join(grandparent_directory, 'data', 'npy')\n",
    "results_directory = os.path.join(grandparent_directory, 'results', 'binary_classification', 'training')\n",
    "\n",
    "if not os.path.exists(results_directory):\n",
    "    os.makedirs(results_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the .json files with the class names\n",
    "with open(os.path.join(data_directory, 'classes.json'), 'r') as file:\n",
    "    classes = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .npy files\n",
    "household_objects = np.load(os.path.join(npy_files_directory, 'household_objects.npy'))\n",
    "animals = np.load(os.path.join(npy_files_directory, 'animals.npy'))\n",
    "\n",
    "# Number of dataset in each class\n",
    "num_household_objects = household_objects.shape[2]\n",
    "print(f'Number of household objects: {num_household_objects}')\n",
    "\n",
    "num_animals = animals.shape[2]\n",
    "print(f'Number of animals: {num_animals}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to normalize the data since the values are already between 0 and 1\n",
    "# Reshape the data\n",
    "household_objects = household_objects.transpose(2, 0, 1)\n",
    "animals = animals.transpose(2, 0, 1)\n",
    "\n",
    "# Print the shape of the data\n",
    "print(f'Household objects shape: {household_objects.shape}')\n",
    "print(f'Animals shape: {animals.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the labels\n",
    "household_objects_labels = np.zeros(num_household_objects)\n",
    "animals_labels = np.ones(num_animals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the data\n",
    "data = np.vstack((household_objects, animals))\n",
    "labels = np.hstack((household_objects_labels, animals_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize the data with a SEED\n",
    "SEED = 42\n",
    "data, labels = shuffle_data(data, labels, SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and testing set in ratio 80:10:10\n",
    "data_train, labels_train, data_val, labels_val, data_test, labels_test = split_data(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the training, validation, and testing set\n",
    "print(f'Training set shape: {data_train.shape}, {labels_train.shape}')\n",
    "print(f'Validation set shape: {data_val.shape}, {labels_val.shape}')\n",
    "print(f'Testing set shape: {data_test.shape}, {labels_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsqueeze the data to add the channel dimension\n",
    "data_train = np.expand_dims(data_train, axis=3)\n",
    "data_val = np.expand_dims(data_val, axis=3)\n",
    "data_test = np.expand_dims(data_test, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input shape\n",
    "input_shape = data_train.shape[1:]\n",
    "output_shape = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the CNN class\n",
    "cnn = CNN(input_shape, output_shape)\n",
    "\n",
    "# Build the model\n",
    "model = cnn.build_model()\n",
    "\n",
    "# Compile the model\n",
    "optimizer = 'adam'\n",
    "loss = 'binary_crossentropy'\n",
    "model = cnn.compile(optimizer, loss)\n",
    "\n",
    "# Print the summary of the model\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of epochs and batch size\n",
    "epochs = 1\n",
    "batch_size = 32\n",
    "\n",
    "# Define the callbacks\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "# Train the model\n",
    "history = cnn.train(data_train, labels_train, data_val, labels_val, epochs, batch_size, callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plots(history, results_directory)\n",
    "loss_plot = plot.loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "print('Saving the model...')\n",
    "model.save(os.path.join(results_directory, 'model.h5'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
